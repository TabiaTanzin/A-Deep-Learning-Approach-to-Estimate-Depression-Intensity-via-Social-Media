{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "622a5dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75fcf5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeUnicode(text):\n",
    "    \"\"\" Removes unicode strings like \"\\u002c\" and \"x96\" \"\"\"\n",
    "    text = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r'', text)       \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r'',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19b8f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceURL(text):\n",
    "    \"\"\" Replaces url address with \"url\" \"\"\"\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','url',text)\n",
    "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3796b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceAtUser(text):\n",
    "    \"\"\" Replaces \"@user\" with \"atUser\" \"\"\"\n",
    "    text = re.sub('@[^\\s]+','atUser',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "682233f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def removeHashtagInFrontOfWord(text):\n",
    "    \"\"\" Removes hastag in front of a word \"\"\"\n",
    "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8337820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def removeNumbers(text):\n",
    "    \"\"\" Removes integers \"\"\"\n",
    "    text = ''.join([i for i in text if not i.isdigit()])         \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55e539d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceMultiExclamationMark(text):\n",
    "    \"\"\" Replaces repetitions of exlamation marks \"\"\"\n",
    "    text = re.sub(r\"(\\!)\\1+\", ' multiExclamation ', text)\n",
    "    return text\n",
    "\n",
    "def replaceMultiQuestionMark(text):\n",
    "    \"\"\" Replaces repetitions of question marks \"\"\"\n",
    "    text = re.sub(r\"(\\?)\\1+\", ' multiQuestion ', text)\n",
    "    return text\n",
    "\n",
    "def replaceMultiStopMark(text):\n",
    "    \"\"\" Replaces repetitions of stop marks \"\"\"\n",
    "    text = re.sub(r\"(\\.)\\1+\", ' multiStop ', text)\n",
    "    return text\n",
    "\n",
    "def countMultiExclamationMarks(text):\n",
    "    \"\"\" Replaces repetitions of exlamation marks \"\"\"\n",
    "    return len(re.findall(r\"(\\!)\\1+\", text))\n",
    "\n",
    "def countMultiQuestionMarks(text):\n",
    "    \"\"\" Count repetitions of question marks \"\"\"\n",
    "    return len(re.findall(r\"(\\?)\\1+\", text))\n",
    "\n",
    "def countMultiStopMarks(text):\n",
    "    \"\"\" Count repetitions of stop marks \"\"\"\n",
    "    return len(re.findall(r\"(\\.)\\1+\", text))\n",
    "\n",
    "def countElongated(text):\n",
    "    \"\"\" Input: a text, Output: how many words are elongated \"\"\"\n",
    "    regex = re.compile(r\"(.)\\1{2}\")\n",
    "    return len([word for word in text.split() if regex.search(word)])\n",
    "\n",
    "def countAllCaps(text):\n",
    "    \"\"\" Input: a text, Output: how many words are all caps \"\"\"\n",
    "    return len(re.findall(\"[A-Z0-9]{3,}\", text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08f1441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with loading all necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25656ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Date</th>\n",
       "      <th>depression_Intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>This is about how festive I feel right now</td>\n",
       "      <td>Depressed</td>\n",
       "      <td>2022-12-23 17:39:14+00:00</td>\n",
       "      <td>0.558321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>How can we deliver order without getting on ...</td>\n",
       "      <td>Depressed</td>\n",
       "      <td>2022-12-23 17:05:55+00:00</td>\n",
       "      <td>0.506055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Not all  are the same and it is important to u...</td>\n",
       "      <td>Depressed</td>\n",
       "      <td>2022-12-23 16:34:07+00:00</td>\n",
       "      <td>0.506055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ÃÂÃÂ¿Sientes que estÃÂÃÂ¡s cayendo mÃÂ...</td>\n",
       "      <td>Depressed</td>\n",
       "      <td>2022-12-23 16:08:03+00:00</td>\n",
       "      <td>0.506055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I am depressed, heartbroken the one who is don...</td>\n",
       "      <td>Depressed</td>\n",
       "      <td>2022-12-23 14:57:10+00:00</td>\n",
       "      <td>0.588954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84598</th>\n",
       "      <td>63925</td>\n",
       "      <td>Slow day. @work. Lots op people have the day o...</td>\n",
       "      <td>Non-Depressive</td>\n",
       "      <td>Fri May 22 02:19:22 PDT 2009</td>\n",
       "      <td>0.344886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84599</th>\n",
       "      <td>63926</td>\n",
       "      <td>@durian_girl Here's a song to celebrate the ar...</td>\n",
       "      <td>Non-Depressive</td>\n",
       "      <td>Sun May 31 03:24:02 PDT 2009</td>\n",
       "      <td>0.444724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84600</th>\n",
       "      <td>63927</td>\n",
       "      <td>From some strange mystical force I got my suit...</td>\n",
       "      <td>Non-Depressive</td>\n",
       "      <td>Sat May 30 20:49:03 PDT 2009</td>\n",
       "      <td>0.313655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84601</th>\n",
       "      <td>63928</td>\n",
       "      <td>skyy vodka-raspberry mixed w/ sprite......my d...</td>\n",
       "      <td>Non-Depressive</td>\n",
       "      <td>Sat May 16 20:29:04 PDT 2009</td>\n",
       "      <td>0.344886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84602</th>\n",
       "      <td>63929</td>\n",
       "      <td>@diancatt te regalo mis 30 free tracks</td>\n",
       "      <td>Non-Depressive</td>\n",
       "      <td>Sun Jun 14 23:32:11 PDT 2009</td>\n",
       "      <td>0.423713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84603 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                              Tweet  \\\n",
       "0               0     This is about how festive I feel right now       \n",
       "1               1    How can we deliver order without getting on ...   \n",
       "2               2  Not all  are the same and it is important to u...   \n",
       "3               3  ÃÂÃÂ¿Sientes que estÃÂÃÂ¡s cayendo mÃÂ...   \n",
       "4               4  I am depressed, heartbroken the one who is don...   \n",
       "...           ...                                                ...   \n",
       "84598       63925  Slow day. @work. Lots op people have the day o...   \n",
       "84599       63926  @durian_girl Here's a song to celebrate the ar...   \n",
       "84600       63927  From some strange mystical force I got my suit...   \n",
       "84601       63928  skyy vodka-raspberry mixed w/ sprite......my d...   \n",
       "84602       63929            @diancatt te regalo mis 30 free tracks    \n",
       "\n",
       "            sentiment                          Date  depression_Intensity  \n",
       "0           Depressed     2022-12-23 17:39:14+00:00              0.558321  \n",
       "1           Depressed     2022-12-23 17:05:55+00:00              0.506055  \n",
       "2           Depressed     2022-12-23 16:34:07+00:00              0.506055  \n",
       "3           Depressed     2022-12-23 16:08:03+00:00              0.506055  \n",
       "4           Depressed     2022-12-23 14:57:10+00:00              0.588954  \n",
       "...               ...                           ...                   ...  \n",
       "84598  Non-Depressive  Fri May 22 02:19:22 PDT 2009              0.344886  \n",
       "84599  Non-Depressive  Sun May 31 03:24:02 PDT 2009              0.444724  \n",
       "84600  Non-Depressive  Sat May 30 20:49:03 PDT 2009              0.313655  \n",
       "84601  Non-Depressive  Sat May 16 20:29:04 PDT 2009              0.344886  \n",
       "84602  Non-Depressive  Sun Jun 14 23:32:11 PDT 2009              0.423713  \n",
       "\n",
       "[84603 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"C:/Users/HP/Desktop/Full_Dataset_depression.csv\", encoding='latin-1')\n",
    "#df['Year'].unique()\n",
    "#df[df['CID']==1]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "011ffbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Basic=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c1a0ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(84603):\n",
    "    text = df['Tweet'].iloc[i]\n",
    "    text = removeUnicode(text)\n",
    "    text= replaceURL(text)\n",
    "    text= replaceAtUser(text)\n",
    "    text = removeHashtagInFrontOfWord(text)\n",
    "    text= removeNumbers(text)\n",
    "    Basic.append(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52170420",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Processed_Text']=Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6923afae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Date</th>\n",
       "      <th>depression_Intensity</th>\n",
       "      <th>Processed_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>This is about how festive I feel right now</td>\n",
       "      <td>Depressed</td>\n",
       "      <td>2022-12-23 17:39:14+00:00</td>\n",
       "      <td>0.558321</td>\n",
       "      <td>This is about how festive I feel right now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>How can we deliver order without getting on ...</td>\n",
       "      <td>Depressed</td>\n",
       "      <td>2022-12-23 17:05:55+00:00</td>\n",
       "      <td>0.506055</td>\n",
       "      <td>How can we deliver order without getting on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Not all  are the same and it is important to u...</td>\n",
       "      <td>Depressed</td>\n",
       "      <td>2022-12-23 16:34:07+00:00</td>\n",
       "      <td>0.506055</td>\n",
       "      <td>Not all  are the same and it is important to u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ÃÂÃÂ¿Sientes que estÃÂÃÂ¡s cayendo mÃÂ...</td>\n",
       "      <td>Depressed</td>\n",
       "      <td>2022-12-23 16:08:03+00:00</td>\n",
       "      <td>0.506055</td>\n",
       "      <td>Sientes que ests cayendo ms y ms en la oscurid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I am depressed, heartbroken the one who is don...</td>\n",
       "      <td>Depressed</td>\n",
       "      <td>2022-12-23 14:57:10+00:00</td>\n",
       "      <td>0.588954</td>\n",
       "      <td>I am depressed, heartbroken the one who is don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84598</th>\n",
       "      <td>63925</td>\n",
       "      <td>Slow day. @work. Lots op people have the day o...</td>\n",
       "      <td>Non-Depressive</td>\n",
       "      <td>Fri May 22 02:19:22 PDT 2009</td>\n",
       "      <td>0.344886</td>\n",
       "      <td>Slow day. atUser Lots op people have the day o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84599</th>\n",
       "      <td>63926</td>\n",
       "      <td>@durian_girl Here's a song to celebrate the ar...</td>\n",
       "      <td>Non-Depressive</td>\n",
       "      <td>Sun May 31 03:24:02 PDT 2009</td>\n",
       "      <td>0.444724</td>\n",
       "      <td>atUser Here's a song to celebrate the arrival ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84600</th>\n",
       "      <td>63927</td>\n",
       "      <td>From some strange mystical force I got my suit...</td>\n",
       "      <td>Non-Depressive</td>\n",
       "      <td>Sat May 30 20:49:03 PDT 2009</td>\n",
       "      <td>0.313655</td>\n",
       "      <td>From some strange mystical force I got my suit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84601</th>\n",
       "      <td>63928</td>\n",
       "      <td>skyy vodka-raspberry mixed w/ sprite......my d...</td>\n",
       "      <td>Non-Depressive</td>\n",
       "      <td>Sat May 16 20:29:04 PDT 2009</td>\n",
       "      <td>0.344886</td>\n",
       "      <td>skyy vodka-raspberry mixed w/ sprite......my d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84602</th>\n",
       "      <td>63929</td>\n",
       "      <td>@diancatt te regalo mis 30 free tracks</td>\n",
       "      <td>Non-Depressive</td>\n",
       "      <td>Sun Jun 14 23:32:11 PDT 2009</td>\n",
       "      <td>0.423713</td>\n",
       "      <td>atUser te regalo mis  free tracks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84603 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                              Tweet  \\\n",
       "0               0     This is about how festive I feel right now       \n",
       "1               1    How can we deliver order without getting on ...   \n",
       "2               2  Not all  are the same and it is important to u...   \n",
       "3               3  ÃÂÃÂ¿Sientes que estÃÂÃÂ¡s cayendo mÃÂ...   \n",
       "4               4  I am depressed, heartbroken the one who is don...   \n",
       "...           ...                                                ...   \n",
       "84598       63925  Slow day. @work. Lots op people have the day o...   \n",
       "84599       63926  @durian_girl Here's a song to celebrate the ar...   \n",
       "84600       63927  From some strange mystical force I got my suit...   \n",
       "84601       63928  skyy vodka-raspberry mixed w/ sprite......my d...   \n",
       "84602       63929            @diancatt te regalo mis 30 free tracks    \n",
       "\n",
       "            sentiment                          Date  depression_Intensity  \\\n",
       "0           Depressed     2022-12-23 17:39:14+00:00              0.558321   \n",
       "1           Depressed     2022-12-23 17:05:55+00:00              0.506055   \n",
       "2           Depressed     2022-12-23 16:34:07+00:00              0.506055   \n",
       "3           Depressed     2022-12-23 16:08:03+00:00              0.506055   \n",
       "4           Depressed     2022-12-23 14:57:10+00:00              0.588954   \n",
       "...               ...                           ...                   ...   \n",
       "84598  Non-Depressive  Fri May 22 02:19:22 PDT 2009              0.344886   \n",
       "84599  Non-Depressive  Sun May 31 03:24:02 PDT 2009              0.444724   \n",
       "84600  Non-Depressive  Sat May 30 20:49:03 PDT 2009              0.313655   \n",
       "84601  Non-Depressive  Sat May 16 20:29:04 PDT 2009              0.344886   \n",
       "84602  Non-Depressive  Sun Jun 14 23:32:11 PDT 2009              0.423713   \n",
       "\n",
       "                                          Processed_Text  \n",
       "0         This is about how festive I feel right now      \n",
       "1        How can we deliver order without getting on ...  \n",
       "2      Not all  are the same and it is important to u...  \n",
       "3      Sientes que ests cayendo ms y ms en la oscurid...  \n",
       "4      I am depressed, heartbroken the one who is don...  \n",
       "...                                                  ...  \n",
       "84598  Slow day. atUser Lots op people have the day o...  \n",
       "84599  atUser Here's a song to celebrate the arrival ...  \n",
       "84600  From some strange mystical force I got my suit...  \n",
       "84601  skyy vodka-raspberry mixed w/ sprite......my d...  \n",
       "84602                 atUser te regalo mis  free tracks   \n",
       "\n",
       "[84603 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "419d7868",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" Creates a dictionary with slangs and their equivalents and replaces them \"\"\"\n",
    "with open('slang.txt') as file:\n",
    "    slang_map = dict(map(str.strip, line.partition('\\t')[::2])\n",
    "    for line in file if line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "515abf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "slang_words = sorted(slang_map, key=len, reverse=True) # longest first for regex\n",
    "regex = re.compile(r\"\\b({})\\b\".format(\"|\".join(map(re.escape, slang_words))))\n",
    "replaceSlang = partial(regex.sub, lambda m: slang_map[m.group(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91151967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countSlang(text):\n",
    "    \"\"\" Input: a text, Output: how many slang words and a list of found slangs \"\"\"\n",
    "    slangCounter = 0\n",
    "    slangsFound = []\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for word in tokens:\n",
    "        if word in slang_words:\n",
    "            slangsFound.append(word)\n",
    "            slangCounter += 1\n",
    "    return slangCounter, slangsFound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "502c906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Replaces contractions from a string to their equivalents \"\"\"\n",
    "contraction_patterns = [ (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'), (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "                         (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), (r'dont', 'do not'), (r'wont', 'will not') ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30283cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def replaceContraction(text):\n",
    "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
    "    for (pattern, repl) in patterns:\n",
    "        (text, count) = re.subn(pattern, repl, text)\n",
    "    return text\n",
    "\n",
    "def replaceElongated(word):\n",
    "    \"\"\" Replaces an elongated word with its basic form, unless the word exists in the lexicon \"\"\"\n",
    "\n",
    "    repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    repl = r'\\1\\2\\3'\n",
    "    if wordnet.synsets(word):\n",
    "        return word\n",
    "    repl_word = repeat_regexp.sub(repl, word)\n",
    "    if repl_word != word:      \n",
    "        return replaceElongated(repl_word)\n",
    "    else:       \n",
    "        return repl_word\n",
    "\n",
    "def removeEmoticons(text):\n",
    "    \"\"\" Removes emoticons from text \"\"\"\n",
    "    text = re.sub(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', '', text)\n",
    "    return text\n",
    "\n",
    "def countEmoticons(text):\n",
    "    \"\"\" Input: a text, Output: how many emoticons \"\"\"\n",
    "    return len(re.findall(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', text))\n",
    "\n",
    "\n",
    "### Spell Correction begin ###\n",
    "\"\"\" Spell Correction http://norvig.com/spell-correct.html \"\"\"\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('corporaForSpellCorrection.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"\"\"P robability of `word`. \"\"\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def spellCorrection(word): \n",
    "    \"\"\" Most probable spelling correction for word. \"\"\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"\"\" Generate possible spelling corrections for word. \"\"\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"\"\" The subset of `words` that appear in the dictionary of WORDS. \"\"\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"\"\" All edits that are one edit away from `word`. \"\"\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"\"\" All edits that are two edits away from `word`. \"\"\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "### Spell Correction End ###\n",
    "\n",
    "### Replace Negations Begin ###\n",
    "\n",
    "def replace(word, pos=None):\n",
    "    \"\"\" Creates a set of all antonyms for the word and if there is only one antonym, it returns it \"\"\"\n",
    "    antonyms = set()\n",
    "    for syn in wordnet.synsets(word, pos=pos):\n",
    "        for lemma in syn.lemmas():\n",
    "            for antonym in lemma.antonyms():\n",
    "                antonyms.add(antonym.name())\n",
    "    if len(antonyms) == 1:\n",
    "        return antonyms.pop()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def replaceNegations(text):\n",
    "    \"\"\" Finds \"not\" and antonym for the next word and if found, replaces not and the next word with the antonym \"\"\"\n",
    "    i, l = 0, len(text)\n",
    "    words = []\n",
    "    while i < l:\n",
    "        word = text[i]\n",
    "        if word == 'not' and i+1 < l:\n",
    "            ant = replace(text[i+1])\n",
    "        if ant:\n",
    "            words.append(ant)\n",
    "            i += 2\n",
    "            continue\n",
    "        words.append(word)\n",
    "        i += 1\n",
    "        return words\n",
    "\n",
    "### Replace Negations End ###\n",
    "\n",
    "def addNotTag(text):\n",
    "    \"\"\" Finds \"not,never,no\" and adds the tag NEG_ to all words that follow until the next punctuation \"\"\"\n",
    "    transformed = re.sub(r'\\b(?:not|never|no)\\b[\\w\\s]+[^\\w\\s]', \n",
    "       lambda match: re.sub(r'(\\s+)(\\w+)', r'\\1NEG_\\2', match.group(0)), \n",
    "       text,\n",
    "       flags=re.IGNORECASE)\n",
    "    return transformed\n",
    "\n",
    "def addCapTag(word):\n",
    "    \"\"\" Finds a word with at least 3 characters capitalized and adds the tag ALL_CAPS_ \"\"\"\n",
    "    if(len(re.findall(\"[A-Z]{3,}\", word))):\n",
    "        word = word.replace('\\\\', '' )\n",
    "        transformed = re.sub(\"[A-Z]{3,}\", \"ALL_CAPS_\"+word, word)\n",
    "        return transformed\n",
    "    else:\n",
    "        return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cba10b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Tweet', 'sentiment', 'Date', 'depression_Intensity',\n",
       "       'Processed_Text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a10bdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b24792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Processed_Text'] = df['Processed_Text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbb047ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(84603):\n",
    "    text = df['Processed_Text'].iloc[i]\n",
    "    #text = countSlang(text)\n",
    "    text= replaceContraction(text)\n",
    "    text= removeEmoticons(text)\n",
    "    #text = replaceNegations(text)\n",
    "    #text= addNotTag(text)\n",
    "    \n",
    "    \n",
    "    pre_processed.append(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4851d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Date</th>\n",
       "      <th>depression_Intensity</th>\n",
       "      <th>Processed_Text</th>\n",
       "      <th>Processed_Text_2nd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>This is about how festive I feel right now</td>\n",
       "      <td>Depressed</td>\n",
       "      <td>2022-12-23 17:39:14+00:00</td>\n",
       "      <td>0.558321</td>\n",
       "      <td>This is about how festive I feel right now</td>\n",
       "      <td>This is about how festive I feel right now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>How can we deliver order without getting on ...</td>\n",
       "      <td>Depressed</td>\n",
       "      <td>2022-12-23 17:05:55+00:00</td>\n",
       "      <td>0.506055</td>\n",
       "      <td>How can we deliver order without getting on ...</td>\n",
       "      <td>How can we deliver order without getting on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Not all  are the same and it is important to u...</td>\n",
       "      <td>Depressed</td>\n",
       "      <td>2022-12-23 16:34:07+00:00</td>\n",
       "      <td>0.506055</td>\n",
       "      <td>Not all  are the same and it is important to u...</td>\n",
       "      <td>Not all  are the same and it is important to u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ÃÂÃÂ¿Sientes que estÃÂÃÂ¡s cayendo mÃÂ...</td>\n",
       "      <td>Depressed</td>\n",
       "      <td>2022-12-23 16:08:03+00:00</td>\n",
       "      <td>0.506055</td>\n",
       "      <td>Sientes que ests cayendo ms y ms en la oscurid...</td>\n",
       "      <td>Sientes que ests cayendo ms y ms en la oscurid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I am depressed, heartbroken the one who is don...</td>\n",
       "      <td>Depressed</td>\n",
       "      <td>2022-12-23 14:57:10+00:00</td>\n",
       "      <td>0.588954</td>\n",
       "      <td>I am depressed, heartbroken the one who is don...</td>\n",
       "      <td>I am depressed, heartbroken the one who is don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84598</th>\n",
       "      <td>63925</td>\n",
       "      <td>Slow day. @work. Lots op people have the day o...</td>\n",
       "      <td>Non-Depressive</td>\n",
       "      <td>Fri May 22 02:19:22 PDT 2009</td>\n",
       "      <td>0.344886</td>\n",
       "      <td>Slow day. atUser Lots op people have the day o...</td>\n",
       "      <td>Slow day. atUser Lots op people have the day o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84599</th>\n",
       "      <td>63926</td>\n",
       "      <td>@durian_girl Here's a song to celebrate the ar...</td>\n",
       "      <td>Non-Depressive</td>\n",
       "      <td>Sun May 31 03:24:02 PDT 2009</td>\n",
       "      <td>0.444724</td>\n",
       "      <td>atUser Here's a song to celebrate the arrival ...</td>\n",
       "      <td>atUser Here is a song to celebrate the arrival...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84600</th>\n",
       "      <td>63927</td>\n",
       "      <td>From some strange mystical force I got my suit...</td>\n",
       "      <td>Non-Depressive</td>\n",
       "      <td>Sat May 30 20:49:03 PDT 2009</td>\n",
       "      <td>0.313655</td>\n",
       "      <td>From some strange mystical force I got my suit...</td>\n",
       "      <td>From some strange mystical force I got my suit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84601</th>\n",
       "      <td>63928</td>\n",
       "      <td>skyy vodka-raspberry mixed w/ sprite......my d...</td>\n",
       "      <td>Non-Depressive</td>\n",
       "      <td>Sat May 16 20:29:04 PDT 2009</td>\n",
       "      <td>0.344886</td>\n",
       "      <td>skyy vodka-raspberry mixed w/ sprite......my d...</td>\n",
       "      <td>skyy vodka-raspberry mixed w/ sprite......my d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84602</th>\n",
       "      <td>63929</td>\n",
       "      <td>@diancatt te regalo mis 30 free tracks</td>\n",
       "      <td>Non-Depressive</td>\n",
       "      <td>Sun Jun 14 23:32:11 PDT 2009</td>\n",
       "      <td>0.423713</td>\n",
       "      <td>atUser te regalo mis  free tracks</td>\n",
       "      <td>atUser te regalo mis  free tracks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84603 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                              Tweet  \\\n",
       "0               0     This is about how festive I feel right now       \n",
       "1               1    How can we deliver order without getting on ...   \n",
       "2               2  Not all  are the same and it is important to u...   \n",
       "3               3  ÃÂÃÂ¿Sientes que estÃÂÃÂ¡s cayendo mÃÂ...   \n",
       "4               4  I am depressed, heartbroken the one who is don...   \n",
       "...           ...                                                ...   \n",
       "84598       63925  Slow day. @work. Lots op people have the day o...   \n",
       "84599       63926  @durian_girl Here's a song to celebrate the ar...   \n",
       "84600       63927  From some strange mystical force I got my suit...   \n",
       "84601       63928  skyy vodka-raspberry mixed w/ sprite......my d...   \n",
       "84602       63929            @diancatt te regalo mis 30 free tracks    \n",
       "\n",
       "            sentiment                          Date  depression_Intensity  \\\n",
       "0           Depressed     2022-12-23 17:39:14+00:00              0.558321   \n",
       "1           Depressed     2022-12-23 17:05:55+00:00              0.506055   \n",
       "2           Depressed     2022-12-23 16:34:07+00:00              0.506055   \n",
       "3           Depressed     2022-12-23 16:08:03+00:00              0.506055   \n",
       "4           Depressed     2022-12-23 14:57:10+00:00              0.588954   \n",
       "...               ...                           ...                   ...   \n",
       "84598  Non-Depressive  Fri May 22 02:19:22 PDT 2009              0.344886   \n",
       "84599  Non-Depressive  Sun May 31 03:24:02 PDT 2009              0.444724   \n",
       "84600  Non-Depressive  Sat May 30 20:49:03 PDT 2009              0.313655   \n",
       "84601  Non-Depressive  Sat May 16 20:29:04 PDT 2009              0.344886   \n",
       "84602  Non-Depressive  Sun Jun 14 23:32:11 PDT 2009              0.423713   \n",
       "\n",
       "                                          Processed_Text  \\\n",
       "0         This is about how festive I feel right now       \n",
       "1        How can we deliver order without getting on ...   \n",
       "2      Not all  are the same and it is important to u...   \n",
       "3      Sientes que ests cayendo ms y ms en la oscurid...   \n",
       "4      I am depressed, heartbroken the one who is don...   \n",
       "...                                                  ...   \n",
       "84598  Slow day. atUser Lots op people have the day o...   \n",
       "84599  atUser Here's a song to celebrate the arrival ...   \n",
       "84600  From some strange mystical force I got my suit...   \n",
       "84601  skyy vodka-raspberry mixed w/ sprite......my d...   \n",
       "84602                 atUser te regalo mis  free tracks    \n",
       "\n",
       "                                      Processed_Text_2nd  \n",
       "0         This is about how festive I feel right now      \n",
       "1        How can we deliver order without getting on ...  \n",
       "2      Not all  are the same and it is important to u...  \n",
       "3      Sientes que ests cayendo ms y ms en la oscurid...  \n",
       "4      I am depressed, heartbroken the one who is don...  \n",
       "...                                                  ...  \n",
       "84598  Slow day. atUser Lots op people have the day o...  \n",
       "84599  atUser Here is a song to celebrate the arrival...  \n",
       "84600  From some strange mystical force I got my suit...  \n",
       "84601  skyy vodka-raspberry mixed w/ sprite......my d...  \n",
       "84602                 atUser te regalo mis  free tracks   \n",
       "\n",
       "[84603 rows x 7 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Processed_Text_2nd'] = pre_processed\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a3891ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from techniques import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38f5e2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocess..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting preprocess..\\n\")\n",
    "\n",
    "\"\"\" Tokenizes a text to its words, removes and replaces some of them \"\"\"    \n",
    "finalTokens = [] # all tokens\n",
    "stoplist = stopwords.words('english')\n",
    "my_stopwords = \"multiexclamation multiquestion multistop url atuser st rd nd th am pm\" # my extra stopwords\n",
    "stoplist = stoplist + my_stopwords.split()\n",
    "allowedWordTypes = [\"J\",\"R\",\"V\",\"N\"] #  J is Adject, R is Adverb, V is Verb, N is Noun. These are used for POS Tagging\n",
    "lemmatizer = WordNetLemmatizer() # set lemmatizer\n",
    "stemmer = PorterStemmer() # set stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c62d14b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(text, wordCountBefore,  y):\n",
    "    totalAdjectives = 0\n",
    "    totalAdverbs = 0\n",
    "    totalVerbs = 0\n",
    "    onlyOneSentenceTokens = [] # tokens of one sentence each time\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    tokens = replaceNegations(tokens) # Technique 6: finds \"not\" and antonym for the next word and if found, replaces not and the next word with the antonym\n",
    "    \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator) # Technique 7: remove punctuation\n",
    "\n",
    "    tokens = nltk.word_tokenize(text) # it takes a text as an input and provides a list of every token in it\n",
    "    \n",
    "### NO POS TAGGING BEGIN (If you don't want to use POS Tagging keep this section uncommented) ###\n",
    "    \n",
    "##    for w in tokens:\n",
    "##\n",
    "##        if (w not in stoplist): # Technique 10: remove stopwords\n",
    "##            final_word = addCapTag(w) # Technique 8: Finds a word with at least 3 characters capitalized and adds the tag ALL_CAPS_\n",
    "##            final_word = final_word.lower() # Technique 9: lowercases all characters\n",
    "##            final_word = replaceElongated(final_word) # Technique 11: replaces an elongated word with its basic form, unless the word exists in the lexicon\n",
    "##            if len(final_word)>1:\n",
    "##                final_word = spellCorrection(final_word) # Technique 12: correction of spelling errors\n",
    "##            final_word = lemmatizer.lemmatize(final_word) # Technique 14: lemmatizes words\n",
    "##            final_word = stemmer.stem(final_word) # Technique 15: apply stemming to words\n",
    "\n",
    "### NO POS TAGGING END ###\n",
    "\n",
    "\n",
    "### POS TAGGING BEGIN (If you want to exclude words using POS Tagging, keep this section uncommented and comment the above) ###          \n",
    "            \n",
    "    tagged = nltk.pos_tag(tokens) # Technique 13: part of speech tagging  \n",
    "    for w in tagged:\n",
    "\n",
    "        if (w[1][0] in allowedWordTypes and w[0] not in stoplist):\n",
    "            final_word = addCapTag(w[0])\n",
    "            #final_word = final_word.lower()\n",
    "            final_word = replaceElongated(final_word)\n",
    "            if len(final_word)>1:\n",
    "                final_word = spellCorrection(final_word)\n",
    "            final_word = lemmatizer.lemmatize(final_word)\n",
    "            final_word = stemmer.stem(final_word)\n",
    "\n",
    "### POS TAGGING END ###\n",
    "                \n",
    "            onlyOneSentenceTokens.append(final_word)           \n",
    "            finalTokens.append(final_word)\n",
    "\n",
    "         \n",
    "    onlyOneSentence = \" \".join(onlyOneSentenceTokens) # form again the sentence from the list of tokens\n",
    "    #print(onlyOneSentence) # print final sentence\n",
    "\n",
    "    \n",
    "    \"\"\" Write the preprocessed text to file \"\"\"\n",
    "    with open(\"result.txt\", \"a\") as result:\n",
    "        result.write(y+\"\\t\"+onlyOneSentence+\"\\n\")\n",
    "        \n",
    "    return finalTokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d44d760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Tweet', 'sentiment', 'Date', 'depression_Intensity',\n",
       "       'Processed_Text', 'Processed_Text_2nd'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb147088",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"ss-twitterfinal.txt\",\"r\", encoding=\"utf8\", errors='replace').read()\n",
    "\n",
    "t0 = time()\n",
    "totalSentences = 0\n",
    "totalEmoticons = 0\n",
    "totalSlangs = 0\n",
    "totalSlangsFound = []\n",
    "totalElongated = 0\n",
    "totalMultiExclamationMarks = 0\n",
    "totalMultiQuestionMarks = 0\n",
    "totalMultiStopMarks = 0\n",
    "totalAllCaps = 0\n",
    "\n",
    "for i in range(84603):\n",
    "   \n",
    "    totalSentences += 1\n",
    "    feat = []\n",
    "    \n",
    "    #textID = (columns[0])\n",
    "    y = (df['sentiment'].iloc[i])\n",
    "\n",
    "    text = removeUnicode(df['Processed_Text'].iloc[i]) # Technique 0\n",
    "    #print(text) # print initial text\n",
    "    wordCountBefore = len(re.findall(r'\\w+', text)) # word count of one sentence before preprocess    \n",
    "    #print(\"Words before preprocess: \",wordCountBefore,\"\\n\")\n",
    "    \n",
    "    text = replaceURL(text) # Technique 1\n",
    "    text = replaceAtUser(text) # Technique 1\n",
    "    text = removeHashtagInFrontOfWord(text) # Technique 1\n",
    "\n",
    "    temp_slangs, temp_slangsFound = countSlang(text)\n",
    "    totalSlangs += temp_slangs # total slangs for all sentences\n",
    "    for word in temp_slangsFound:\n",
    "        totalSlangsFound.append(word) # all the slangs found in all sentences\n",
    "    \n",
    "    text = replaceSlang(text) # Technique 2: replaces slang words and abbreviations with their equivalents\n",
    "    text = replaceContraction(text) # Technique 3: replaces contractions to their equivalents\n",
    "    text = removeNumbers(text) # Technique 4: remove integers from text\n",
    "\n",
    "    emoticons = countEmoticons(text) # how many emoticons in this sentence\n",
    "    totalEmoticons += emoticons\n",
    "    \n",
    "    text = removeEmoticons(text) # removes emoticons from text\n",
    "\n",
    "    \n",
    "    totalAllCaps += countAllCaps(text)\n",
    "\n",
    "    totalMultiExclamationMarks += countMultiExclamationMarks(text) # how many repetitions of exlamation marks in this sentence\n",
    "    totalMultiQuestionMarks += countMultiQuestionMarks(text) # how many repetitions of question marks in this sentence\n",
    "    totalMultiStopMarks += countMultiStopMarks(text) # how many repetitions of stop marks in this sentence\n",
    "    \n",
    "    text = replaceMultiExclamationMark(text) # Technique 5: replaces repetitions of exlamation marks with the tag \"multiExclamation\"\n",
    "    text = replaceMultiQuestionMark(text) # Technique 5: replaces repetitions of question marks with the tag \"multiQuestion\"\n",
    "    text = replaceMultiStopMark(text) # Technique 5: replaces repetitions of stop marks with the tag \"multiStop\"\n",
    "\n",
    "    totalElongated += countElongated(text) # how many elongated words emoticons in this sentence\n",
    "    \n",
    "    tokens = tokenize(text, wordCountBefore,  y)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837fd4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "print(\"Total sentences: \",totalSentences,\"\\n\")\n",
    "print(\"Total Words before preprocess: \",len(re.findall(r'\\w+', f)))\n",
    "print(\"Total Distinct Tokens before preprocess: \",len(set(re.findall(r'\\w+', f))))\n",
    "print(\"Average word/sentence before preprocess: \",len(re.findall(r'\\w+', f))/totalSentences,\"\\n\")\n",
    "print(\"Total Words after preprocess: \",len(tokens))\n",
    "print(\"Total Distinct Tokens after preprocess: \",len(set(tokens)))\n",
    "print(\"Average word/sentence after preprocess: \",len(tokens)/totalSentences,\"\\n\")\n",
    "\n",
    "\n",
    "print(\"Total run time: \",time() - t0,\" seconds\\n\")\n",
    "\n",
    "print(\"Total emoticons: \",totalEmoticons,\"\\n\")\n",
    "print(\"Total slangs: \",totalSlangs,\"\\n\")\n",
    "commonSlangs = nltk.FreqDist(totalSlangsFound)\n",
    "for (word, count) in commonSlangs.most_common(20): # most common slangs across all texts\n",
    "    print(word,\"\\t\",count)\n",
    "\n",
    "commonSlangs.plot(20, cumulative=False) # plot most common slangs\n",
    "\n",
    "print(\"Total elongated words: \",totalElongated,\"\\n\")\n",
    "print(\"Total multi exclamation marks: \",totalMultiExclamationMarks)\n",
    "print(\"Total multi question marks: \",totalMultiQuestionMarks)\n",
    "print(\"Total multi stop marks: \",totalMultiStopMarks,\"\\n\")\n",
    "print(\"Total all capitalized words: \",totalAllCaps,\"\\n\")\n",
    "\n",
    "#print(tokens)\n",
    "commonWords = nltk.FreqDist(tokens)\n",
    "print(\"Most common words \")\n",
    "print(\"Word\\tCount\")\n",
    "for (word, count) in commonWords.most_common(100): # most common words across all texts\n",
    "    print(word,\"\\t\",count)\n",
    "\n",
    "commonWords.plot(100, cumulative=False) # plot most common words\n",
    "\n",
    "\n",
    "bgm = nltk.collocations.BigramAssocMeasures()\n",
    "tgm = nltk.collocations.TrigramAssocMeasures()\n",
    "bgm_finder = nltk.collocations.BigramCollocationFinder.from_words(tokens)\n",
    "tgm_finder = nltk.collocations.TrigramCollocationFinder.from_words(tokens)\n",
    "bgm_finder.apply_freq_filter(5) # bigrams that occur at least 5 times\n",
    "print(\"Most common collocations (bigrams)\")\n",
    "print(bgm_finder.nbest(bgm.pmi, 50)) # top 50 bigram collocations\n",
    "tgm_finder.apply_freq_filter(5) # trigrams that occur at least 5 times\n",
    "print(\"Most common collocations (trigrams)\")\n",
    "print(tgm_finder.nbest(tgm.pmi, 20)) # top 20 trigrams collocations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed44df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dff = pd.read_csv('result.txt', delimiter = \"\\t\",encoding='latin-1',names = ['column1_name','column2_name'])\n",
    "dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528d5614",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Final_Text']=dff['column2_name']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab38d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.rename(columns={'sum': 'Average'})\n",
    "\n",
    "df2.to_csv(\"Processed_full_Depression.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da119f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Processed_full_Depression.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117757e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
